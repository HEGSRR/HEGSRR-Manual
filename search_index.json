[["index.html", "HEGSRR Manual Chapter 1 About 1.1 Usage 1.2 Render book 1.3 Preview book", " HEGSRR Manual Peter Kedron Joseph Holler 2023-07-18 Chapter 1 About This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports; for example, a math equation \\(a^2 + b^2 = c^2\\). 1.1 Usage Each bookdown chapter is an .Rmd file, and each .Rmd file can contain one (and only one) chapter. A chapter must start with a first-level heading: # A good chapter, and can contain one (and only one) first-level heading. Use second-level and higher headings within chapters like: ## A short section or ### An even shorter section. The index.Rmd file is required, and is also your first book chapter. It will be the homepage when you render the book. 1.2 Render book You can render the HTML version of this example book without changing anything: Find the Build pane in the RStudio IDE, and Click on Build Book, then select your output format, or select “All formats” if you’d like to use multiple formats from the same book source files. Or build the book from the R console: bookdown::render_book() To render this example to PDF as a bookdown::pdf_book, you’ll need to install XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. 1.3 Preview book As you work, you may start a local server to live preview this HTML book. This preview will update as you edit the book when you save individual .Rmd files. You can start the server in a work session by using the RStudio add-in “Preview book”, or from the R console: bookdown::serve_book() "],["reproducible-research-compendia-and-workflows.html", "Chapter 2 Reproducible research compendia and workflows 2.1 Open science, reproducibility and replicability 2.2 Pragmatic principles for designing reproducible research workflows 2.3 Organization / Structure of this", " Chapter 2 Reproducible research compendia and workflows Introduction paragraph Other analogous reproducibility infrastructure projects include: TIER protocol: https://www.projecttier.org/tier-protocol/protocol-4-0/ Social Science Reproduction Platform https://www.socialsciencereproduction.org/ and Guide for Accelerating Computational Reproducibility in the Social Sciences: https://bitss.github.io/ACRE/ WORCS A workflow for open reproducible code in science https://cjvanlissa.github.io/worcs/ 2.1 Open science, reproducibility and replicability Theoretical foundation for open science practices … discovering valid and generalizable knowledge. Open science is… Mertonian norms (1942) (christensen et al pg 23) “Replication is a methodological tool based on a reptition procedure that is invovled in establishing a fact, truth, or piece of knowledge.” (Schmidt 20019) Reproducibility is… An original study may be considered computationally reproducible if… A reproduction study… Replicability is… A replication study… (Christensen et al pg 159) Same Procedure Difference Procedure Same Data Reproduction Renalysis Different Data Replication Extension 2.2 Pragmatic principles for designing reproducible research workflows How did we decide how to construct the research template and workflow? We followed these principles in making design decisions: Efficiency for both individual researchers and research teams Flexibility to accommodate many types of research Transparency and provenance at both the level of individual research artefacts and the overall research design Compatibility with other research standards FAIR data Lower transaction costs Ease of appropriate citation for research products 2.2.1 Efficiency Individual tasks will start to take more time, especially at first, but in the long-term, the amount of time and effort put into the research project from inception to publishing will save effort. In particular, researchers will start to reap dividends when they need to revise research, verify methodology and results for publication, or revise or correct errors early in the research workflow. Adopting reproducible research practices also reduces the transaction costs of pausing a research project (e.g. for a field season or heavy teaching semester) and restarting later. Reduce inefficiencies in research teams, especially teams distributed over space (in different research labs or at home and in the field) or time (e.g. shifting composition of a research team as students come and go from the program) 2.2.2 Flexibility diversity in geography complexity and heterogeneity of phenomena (NASEM complexity ; precision of replication) not just computational geography - qualitative and mixed methods research as well original studies, meta-analyses, reproductions, replications ability to fork a reproduction into a replication 2.2.3 Transparency and Provenance applies to individual research products, e.g. data, code, resulting figures or tables also applies to the overall research project in terms of research design, suggesting the need for pre-analysis registrations, tracking changes in research design 2.2.4 Ethics and Privacy research workflow and template must account for appropriate steps to protect the privacy of human subjects and adhere to standards and (Institutional review boards (IRB) / independent ethics committees (IEC) / etc.) reviews for responsible and ethical research. 2.2.5 Compatibility with OSF, with journal &amp; funding agency requirements, with standardized metadata facilitates convergence research if this is interoperable with other disciplines 2.2.6 FAIR Data The FAIR principles of findability, accessibility, interoperability, and reusability open standards (data formats) licensing metadata documented with open standards 2.2.7 Transaction costs economic definition of transaction costs, maximizing legibility of codified knowledge. Maximizing efficiency and transparency of research communication and re-use / extensibility, both for other researchers and for students. The FAIR principles of findability, accessibility, interoperability, and reusability of FAIR data contribute substantially to reducing the transaction costs for other researchers or computer algorithms to access and use the research project. However addition adherence to a common organizational structure/template for research compendium (think of principles for communication in education– standardize format &amp; structure as much as possible) open source software computational environment legible code modularity 2.2.8 Citation Once your work is easily reusable and extensible, you will want credit for it. DOI’s, Citation file in Git Repository, possibly DOI for particular artefacts if you anticipate citing them. Cross-linking different research products so that each project becomes an living ecosystem of research components. 2.3 Organization / Structure of this Template itself and recommended content Research workflow and recommended actions We organize this manual to follow a research workflow of recommended actions, including actions interacting with, creating and editing the research compendium. We follow the open science workflow composed of steps for: Provocation Ideation Data observation and curation (“Knowledge Generation” in NASEM 2018) Validation Dissemination Preservation "],["provocation.html", "Chapter 3 Provocation 3.1 Literature review 3.2 Reading a research compendium 3.3 Literature for reproduction, reanalysis, or replication 3.4 Data at the provocation phase 3.5 Setting up research compendium 3.6 Conclusion", " Chapter 3 Provocation What do you have as you enter the provocation phase? Interest in a topic… Framework for discovering and proving causal mechanisms, and extending this to consider types of research questions and motivations for extensions, reanalyses, replications, or reproductions. Include exploratory analysis and case studies in type of research. Reference: www.edreplication.org 3.1 Literature review create bibtex file for manuscript beyond the manuscripts: pay attention to references to software, code, data, supplementary materials use reference manager to collect references for ease of bibtex generation take notes on search date, keywords, etc. include preprints literature reviews become more powerful with open science 3.2 Reading a research compendium give idea of end goal at the beginning by reading one of our finished research compendium cite Nust’s paper on reading a research compendium If you are running a study written in R using the groundhog package for reproducibility, you may need to run that console or enter “OK” in the console to permit groundhog to install packages for the study to run. 3.3 Literature for reproduction, reanalysis, or replication First, how will you choose a study? Some criteria for choosing studies include: - intellectual merit and the study’s influence on a field of study - broader impacts on society or the environment, e.g. through influence on policy - feasibility based on availability of materials and methods - desire to learn or teach the study and its methods - concerns about the validity of a study We argue that any reproduction, reanalysis, or replication starts with assembling and synthesizing the most complete information available about the original study design. In order to reproduce a prior study, the reproducing researchers or students will need a comprehensive understanding of the original study. We recommend two approaches for systematically approaching a deep reading of the original study resulting in comprehensive understanding and digital resources that link that understanding to the original manuscript and supplementary materials. These approaches may be undertaken in parallel, and include: Annotate the original manuscript and supplementary materials Draw a workflow plan for the original study 3.3.1 Annotation We recommend annotating in collaborative software environments, e.g. hypothes.is or Zotero. Choose a system of colors to represent important components of the research design, including: Main hypotheses or research questions Theoretical concepts Inputs of data or observations Processes and parameters for processes Computational environment Results, especially any result that can be checked or compared You may also apply tags to indicate the relationship between individual hypotheses or questions and their associated processes and results and to indicate the sequence of processes in the overall research workflow. As you approach a final draft of the workflow plan (see next subsection), we recommend assigning a unique identifier to each process and tagging the relevant sections of the original manuscript and supplementary materials with that identifier, creating a link between the original manuscript and the workflow plan. 3.3.2 Workflow plan The workflow visually illustrates the research process with three types of symbols: Ovals represent inputs and outputs of databases, variables, observations, etc. Rectangles represent processes, algorithms, models, etc. Arrows illustrate the sequence and flow of the research design Early drafts of the workflow plan may be digital (e.g. on a digital whiteboard, or simply a text outline) or analog and tactile. In methods classes, Holler has used stacks of notecards, marker pens and tape to arrange workflows on a board. We even printed results from the paper (e.g. tables, graphs, and maps) to use as nodes of the analog board-based workflow graph. Remote classes or research teams could do the same with images on a digital whiteboard. We recommend drawing final draft of the workflow plan in specialized software for this task, e.g. draw.io or ____. 3.4 Data at the provocation phase Avoid biasing your research plan by interacting too much with data observations record / journal any empirical observations / interactions with data view descriptive statistics and data visualization Save metadata 3.5 Setting up research compendium 3.5.1 what is a research compendium, and why start one so early? 3.5.2 why make your research compendium in Git? In our experience, there are at least XXX compelling reasons to use some form of git version control for your research compendium. First, Git allows you to track, control, and revert versions of your own work, giving you an undo feature for your entire research project. But it’s more powerful than a simple undo feature, because Git also allows you to browse the history of versions, and visualize the changes in each version, and selectively restore elements of previous versions. You control exactly when to commit new versions and describe the purpose of the new versions. Advanced users can even experiment with branching multiple alternative versions of the project, e.g. to sandbox a thread of alternative changes to the project without altering the main version and then either discard failed changes or to decide when and how to integrate or merge the alternatives into the main project. Second, Git allows you to manage updates to the project in a research team, by allowing each team member to commit changes sequentially, or by allowing team members to check out a branch of the project to work on a project subcomponent until they are ready to merging their contributions back into the main project. If research team members work this way, the provenance of individual contributions to the team project will be recorded in git commit metadata. If a research team uses a service like GitHub or GitLab to host the Git repository, then the research team will have access to online management and collaboration tools linked to the repository for managing, tracking, and communicating about the research project. Third, Git and services providing servers for Git (e.g. GitHub and GitLab) make it easy to share projects with other researchers and with other computational environments. The research compendium can be shared with other researchers via a URL link for cloning the repository to a new computer or manually downloading a compressed zip archive file of the whole compendium. The compendium can also be cloned into a new computational environment, e.g. into an RStudio Server, JupyterHub Server, high performance computing (HPC) service, or Docker container. This movement of the full research compendium between computational environments is an essential component for computationally-intensive research and for computational reproducibility. Fourth, because Git maintains a history of commits or versions to the project, it allows the researcher to passively record the full provenance of a study’s development and revision over time, including information about what changes were made, by whom, when, and for what reasons. This adds transparency to the revision process, e.g. by making explicit how the research changed through peer review and what each member of a research team contributed. Fifth, GitHub and GitLab can be integrated with digital archives for open science, e.g. Open Science Foundation or Figshare. They also support web services with which researchers may publish webpages with their findings directly within their research compendium Finally, Git is highly compatible with an open science paradigm of research in which research projects become “living papers” that may be updated over time by the original authors, editors, peer reviewers, or students. Other researchers may easily fork new versions of the project for the purpose of either making and proposing changes to the original research in the form of a pull request, or for developing the project into a new metascience study while maintaining a formal link to the original study for the purposes of receiving updates or tracking provenance of research designs. 3.5.3 Create new Git repository based on our template public / private status changes are tracked with commits Orientation to the template structure / file system top-level readme license Markdown language metadata docs / reports committing some changes to the template Recommended prefixes: Or / RPl / RPr Choose appropriate LICENSE file for the work 3.6 Conclusion At the conclusion of the provocation phase, you have: Specific research question Connect research question to literature and to archetypical research design Set up template research compendium "],["ideation-and-research-design.html", "Chapter 4 Ideation and Research Design 4.1 Develop your Research Plan 4.2 Project metadata: front matter for your study 4.3 Create an OSF project 4.4 Data in the ideation phase 4.5 Writing 4.6 Citing 4.7 Procedures 4.8 Pilot study 4.9 Conclusion", " Chapter 4 Ideation and Research Design In the provocation phase, you already developed a specific research question, chose an archetypical research design, and set up a research compendium. Now it is time to develop your research plan! Remember, you can amend your research plan later. You can even embargo your research plan and git repository for now before making it public later. The point is to start increasing the transparency and provenance of your research project now, so that it can be made public later. 4.1 Develop your Research Plan It is already time to translate your specific research question and archetypical research design, into a specific research plan. 4.1.1 Reproduction studies 4.1.2 Reanalysis studies 4.1.3 Replication studies 4.1.4 Original studies 4.2 Project metadata: front matter for your study Every journal article and book starts with front matter, and this is essentially metadata about the publication, including citation and attribution, copyright license, distribution, and more. Open science starts with making your research project findable, which will require specifying standard metadata describing your research project. Once you take the time to describe this project metadata, you will likely find that you continue to reuse the same information about your research throughout the full life cycle. This is no coincidence, as many publishers and repositories have adopted the same DUBLIN Core standards https://www.dublincore.org/specifications/dublin-core/ for describing research artifacts. Therefore, it is never too early to create a working draft of your project metadata in the top readme file of your research compendium. This same information will be transferred to your analysis plans and research reports. Some research designs may have multiple subcomponents with different spatio-temporal characteristics. However, for this project-level metadata, simply report one all-encompassing set of spatio-temporal coverage metadata inclusive of all subcomponents and at the finest resolution. Some research designs may be a replication study with plans to alter the prior study’s spatio-temporal characteristics, in which case this project-level metadata for the research compendium should report the planned spatio-temporal coverage for the replication study. Description: a brief abstract about your research project. Contributors: names and email contact information for contributors. Note the corresponding author with an asterisk *. This corresponding author should be considered the creator of Dublin Core metadata. On GitHub, you may want to explicitly link to contributors’ GitHub profiles with the @ symbol, e.g. @josephholler. On OSF, you will be asked to specify contributors’ permissions (administrator, read &amp; write, or read) and whether they are a bibliographic or non-bibliographic contributor. Affiliated Institutions: List affiliations of the contributors as one would do for publications. On OSF, this field auto-populates with the registered institutions of any of the contributors as a benefit to institutional subscriptions. Funding: If applicable, include information on one or more funding sources for the research project. Funder Name (for NSF project, the specific division or directorate) Award Title Award info URI (web address) Award number Date created: date when project was started Date modified: date of most recent revision Subject: select from a controlled vocabulary Tags: select a few keywords, separated by commas. This is a feature compatible with GitHub, OSF, and Journal Articles, but is not a Dublin Core element. Coverage (geographic): Specify the geographic extent of your study. This may be a place name and link to a feature in a gazetteer like GeoNames or OpenStreetMap, or a well known text (WKT) representation of a bounding box. Coverage (temporal): Specify the temporal extent of your study—i.e. the range of time represented by the data observations. Relation: Identifier links to other related research elements, e.g. the OSF repository, preprints, Rights: Maintain the link to a license file and also list the license type here. Resource type: The overall research compendium is a Collection which will eventually include more specific subcomponents, e.g. Preprint, or JournalArticle. Dublin Core maintains a list of resource types: https://www.dublincore.org/specifications/dublin-core/resource-typelist/ as does OSF: https://help.osf.io/article/570-resource-types-in-osf Resource language: most likely English Conforms To: Conforms to the research compendium template for reproducible and replicable human-environment and geographical sciences at https://hegsrr.github.com/HEGSRR-Template The project metadata has excluded just a few Dublin Core elements: - Title is implied by the first-level header of the Markdown document. - Creator is implied by the contributor noted as the corresponding author and by the history of Git commits. - Publisher will likely include multiple entities as the repository linked to services like GitHub, GitLab, OSF, Figshare, or other digital Git repositories or archives. Publisher will be implied by the service on which the service is hosted. - Format is excluded because the research compendium collection contains digital files of many different formats. The metadata also includes a few elements beyond the 15 core elements: - Coverage is explicitly subdivided into geographic and temporal coverages. - Funding is added for compatibility with OSF and to fulfill requirements of funding agencies. - Affiliated Institutions is added for compatibility with OSF and would normally be included with 4.3 Create an OSF project The Open Science Foundation (OSF, at https://osf.io) is a digital archive for open science digital artifacts. We recommend creating an umbrella OSF project mapping directly to your Git research compendium. First, create new project. Make the project settings of title and description identical to the information in your compendium readme.md file. The best category value for a complete research compendium is project. The other project categories are more suitable for specific project components, should you need to register those separately. Second, add additional contributors, unless you are developing the project as a solo researcher. For each contributor, choose their level of permissions (administer, read and write, or just read) and check the bibliographic contributor if they are to be included in citations to this work. Third, the project License can be set on the project home page (click the project title at top-left) under the Description. Use the Add a license link to choose a license. Stodden (XXXX) suggests the BSD 3-Clause “New”/“Revised” License for computational open science research. If you do not choose a license or if you choose “No license”, then full copyright protections are implied and it will be impossible for other researchers to re-use the project content. Fourth, complete additional metadata. The description field points to the same data as the project settings description and the contributors data points to the same data as the contributors menu. The affiliated institutions are controlled by OSF institutional subscriptions, and the date created and date modified are automatically generated. This leaves three metadata fields to manage: - Resource information: The Resource type should be set to Collection for a research compendium composed of multiple resource types and data formats. Resource language should likely be English, unless you have translated the compendium template. Other project subcomponents could be registered separately with more specific resource type classifications. - Funding/support information: The funding fields match information in your compendium readme.md file. - Tags: match information in your compendium readme.md file. Fifth, connect your OSF project to your GitHub repository. - Add-ons -&gt; GitHub - Files -&gt; add repository??? As you continue to work on your git repository and push changes to GitHub, your OSF project will automatically stay up-to-date with all of the changes. Further on in the research life cycle you will create OSF registrations, which will archive a static version of your compendium at the time of registration. Finally, projects have a Make Public option, which OSF warns you not to use until you are ready for the contents to be truly public. When you are ready for the project to be public, you can also generate a DOI for inclusion in your project Citation. Additionally, you can type bibtex into the get more citations dropdown to copy BibTex citation data for your OSF project. Once your project is public with a DOI, you may crosslink this information from the GitHub repository: - Return to the compendium readme.md and add the OSF project DOI to the Related to: section. - Update the CITATION.cff file to cite your OSF Project with DOI. 4.4 Data in the ideation phase At this phase of research, we should avoid directly observing study data to the greatest extent that this can be avoided. The pre-analysis plan will require you to disclose any experience or engagement you have had with the data because of the potential biases this engagement introduces into the research process. Therefore, our larger discussion about data management with open science research compendiums is in the next chapter on Observation. At this stage, we should be more concerned about researching potential secondary data sources and inventing the structure and observation design for primary data to be collected. As such, we are more concerned at this phase about metadata about the data we are to acquire, create, and analyze, than we are about the data themselves. It is recommended to save information about your data into the data/metadata folder, to start populating the data/data_metadata.csv tabular index to the data directories with a row for each data sources and associated metadata files, and possibly to write preliminary code for accessing data with specialized packages or application programming interfaces (APIs), including using APIs to query and save metadata. The data_metadata.csv file may contain information about data files that do not yet exist At this phase of research, you may not know specific paths and names for the files you will create. However, you will want to keep track of the different layers you intend to create or acquire and their associated metadata files. Therefore, you may create temporary working names for the name column, keep an accurate list of metadata files in the metadata column, n The data_metadata.csv file is a tabular index of each data file in your project, including the fields: path: the path to the data folder, likely one of: raw\\private, raw\\public, derived\\private or derived\\public name: the file name, including extension. You may refer to individual tables of relational databases (e.g. geopackages) by appending | and the layer name, e.g. the my_layer table of my_geopackage.gpkg could be noted with this name: my_geopackage.gpkg|my_layer metadata: list of metadata files for this data source, stored in the data\\metadata folder. These may include ISO-191** or FGDC standard XML files, data dictionaries, licenses or attributions, user guides, webpage printouts, etc. Separate multiple files with semicolons: ;. description: very brief description of the dataset. If the data is simulated, randomized, or represents only a limited sample of the full research dataset, you should note those limitations here. In many cases you may document metadata about data sources that will not be directly included in the repository because they are either too large, they are proprietary, or they contain confidential information. The data_metadata.csv file should proscribe the location of these types of data as raw\\private for data sources that need to be downloaded and/or acquired with permission and/or funding or as derived\\private for large or confidential files that will be created with computational code or scripts. Researchers are strongly encouraged to include additional metadata in the metadata folder. Further information about the procedures used to create or acquire data to be stored in private directories should be maintained in the procedure_metadata.csv. Ideally, metadata should be documented to FGDC or ISO standards and saved in xml, json, or yaml format. Some tools for reading and writing metadata include: USGS Metadata Wizard 2.0: https://www.usgs.gov/software/metadata-wizard-20 MDEditor: https://www.mdeditor.org/ GeoNetwork: https://geonetwork-opensource.org/ R geometa: https://cran.r-project.org/web/packages/geometa Python pygeometa: https://pypi.org/project/pygeometa/ Social science metadata may follow the Data Documentation Initiative (DDI) Codebook: https://www.projecttier.org/tier-protocol/protocol-4-0/root/data/originaldata/metadata/metadataguide/ and Project TIER provides good advice about data metadata: https://www.projecttier.org/tier-protocol/protocol-4-0/root/data/originaldata/metadata/. 4.5 Writing Write in plain text or code new sentence on each line stored in Git repository with version control options: non-computational (markdown or latex only) Rmarkdown computational notebook Jupyter Python We need to move toward having the pre-analysis plan, code, outputs, everything in the same R markdown file or Jupyter notebook. For Markdown, use knitr to generate the report https://yihui.org/knitr/ For Jupyter, use stitch https://pystitch.github.io/ When knitr or stitch is run, the report can be saved to the reports folder. At that time, the researcher should commit a Version of the Git repository, and then continue revising the main notebook / rmarkdown. This will reduce redundancies while recording provenance of the data, research plan, and code. 4.6 Citing Use BibTex for managing citations for integration with Rmarkdown and LaTeX Necessary to cite not only literature, but also software and data Important to give credit to researchers who developed the software packages and/or data you are using by citing their preferred reference GitHub repositories may have .cff files or other information with preferred citations Citations to other versions of the paper and work (e.g. preprint, compendium, data, code) citation package in R https://ropensci.org/blog/2021/11/16/how-to-cite-r-and-r-packages/ Guide to intergating Zotero and RMarkdown: https://christopherjunk.netlify.app/blog/2019/02/25/zotero-rmarkdown/ The worcs package in r supports distinguishing “essential” references from “non-essential” references, so that a full reference list can be rendered in a preprint while a limited reference list can be rendered in a journal article with limitations on the number of references or words. See https://cjvanlissa.github.io/worcs/articles/citation.html 4.7 Procedures Save all methods and procedures and supporting documents in the procedures directory. Catalog all procedures in an ordered table documenting any code or other research procedure/protocol documents. Provide a brief description of the purpose of each procedure and piece of code. Catalog the files in procedure_metadata.csv See the example table below, and modify the table to suit your research design. path: the path to the file or directory, usually one of code for software code and scripts, environment for the hardware/software computational environment, or protocol for non-code protocols like name: the file name, including extension purpose: very brief description of the purpose of the file The sequence of procedures to be followed is implied by the order in the table and should be explicit in the pre-analysis plan and post-analysis report. path name purpose code script1.R download and preprocess data protocol survey_irb.pdf Institutional review board protocol for survey sampling and instrument protocol mapworkshop.pdf participatory mapping workshop protocol code script2.R run analysis code script3.R generate visualizations for results 4.7.1 Environment Store detailed information about the hardware and software environment requirements for procedures and code here. You may also document a recipe or container of the computational environment here. This directory is specifically for hardware and software environments. Contextual factors or confounds of human subjects research or field research should be communicated in protocol documents and stored in the protocols subdirectory. It is imperative to track the versions of software, software packages, and their dependencies for a research project. Package versions change frequently, and may even change while you are working on your project. We highly recommend keeping one main list of all software packages and versions requried for a project and using this list to install and load the packages in code and to document metadata about the computational environment. Many templates and guides to computational research suggest writing code in a series of scripts, one for each phase of the research workflow. If you are taking this approach, beware of loading different sets of packages in different scripts without maintaining one comprehensive list of all required packages. For users of R, our template code, at a minimum, saves environment information using the sessionInfo() function. Our Rmarkdown template includes code for using the groundhog package to manage loading the version of R and packages and their dependencies as they were on a specific date. For users of Python, … 4.7.2 Code Store computational code-based research procedures in the code subdirectory. 4.7.3 Protocols Store any non-computational protocols and research procedures in the protocols subdirectory. store IRB protocol and any forms or instruments in procedures / protocols the research compendium is part of the data management plan read ahead to dissemination phase for considering privacy and data management 4.8 Pilot study Some research designs will benefit from a pilot study, which will use a limited set of observations to test and revise (the) data collection instrument(s) or test the methodology on a limited set of data. Pilot studies should be specified in the analysis plan methodology and disclosed in the Prior observations section of the plan. The data observations and findings from a pilot study are not be included in the complete study and will not be disseminated or archived. Therefore, pilot study data and materials may be saved in the data/scratch directory for temporary use. should pilot study data go into the /scratch folder? 4.9 Conclusion You have developed your archetypical research design into a specific registered research plan. You have developed your research compendium You have connected your research registration on OSF to a GitHub "],["writing-an-analysis-plan-for-preregistration.html", "Chapter 5 Writing an Analysis Plan for Preregistration 5.1 Study metadata 5.2 Study design 5.3 Planned deviations of metascience studies 5.4 Preregister your analysis plan on OSF 5.5 Amending the analysis plan registration 5.6 Resources", " Chapter 5 Writing an Analysis Plan for Preregistration Pre-analysis plan templates were designed to help researchers specify the research design decisions most susceptible to p-hacking in their disciplines. It follows intuitively that the templates address research issues in biomedical research, psychology, econometrics, and other disciplines at the forefront of discovering and addressing the “replication crisis”. The templates are well suited to experimental and quasi-experimental research designs in which the researcher has significant control over sampling and observations. However, we have not found pre-analysis plans the human-environment and geographical sciences or related disciplines, in which many research designs integrate many different data sources with different spatio-temporal supports. These research designs require sophisticated methods involving many researcher decisions and multiple threats to validity: therefore they require attention and detail in any analysis plan for preregistration. At the same time, we recognize that other disciplines look to geography when it comes time to specify details about their (spatial) data sources, referring researchers to the Federal Geographic Data Committee (FGDC) or the ISO 191* series of standards for geographic metadata. Furthermore, the highest standards for reproducible research compendia and for research data archiving require documentation with international standards. Therefore, we have adopted the Dublin Core standard with expanded detail of the coverage element to describe the spatio-temporal support of the overall project; and we have adopted the International Standards Organization (ISO) 19115 standard for geographic information metadata to describe the spatio-temporal support of individual data sources. The next sections of this chapter will guide you through the steps for preparing the analysis plan for preregistration, culminating with instructions on registering the plan on OSF. 5.1 Study metadata The analysis plan begins with the title and metadata for the project. If you have completed the root readme.md file for your research compendium, you can probably copy that metadata directly into the top of your analysis plan. This redundancy is necessary so that the registered plan document contains all necessary information independently from the research compendium. This project-level metadata also provides the foundation of the analysis plan by specifying the spatio-temporal characteristics of the study, making explicit from the beginning what the target data support will be for any of the input data. Please see instructions on this overarching metadata in Ideation Chapter, Section X, and see contingencies for studies with multiple sub-components and replication studies below. 5.1.1 Studies with multiple spatio-temporal coverages Some studies may contain subcomponents with different spatio-temporal characteristics, or may contain multi-level models. In this case, you will need to enumerate different metadata values for the spatial and/or temporal characteristics that vary across subcomponents of the study. The list of study metadata should contain the overarching metadata for the study (i.e. coverages inclusive of all subcomponents and resolutions as ranges from smallest to largest). Below the main metadata fields, make a hierarchical list or a table of the study subcomponents and the metadata characteristics that vary across them. For example, in a study of Social Vulnerability Indices across different spatial extents, the overarching metadata may be: - Spatial Coverage: Continental United States (this spatial extent encompasses all subcomponents of the study) - Spatial Resolution: Counties to EPA Regions (range from smallest resolution or enumeration units to largest) - Spatial Reference System: ESRI:102003 (spatial reference system for the overarching spatial coverage) - Temporal Coverage: 2017-2022 (this temporal extent encompasses all subcomponents of the study) - Temporal Resolution: 5-year estimate (this example has only one resolution and therefore does not require a range) The spatial coverage, resolution, and reference system can be shown to vary across study subcomponents by adding a statement explaining the subcomponents, followed by a hierarchical list. For example: There are three subcomponents to the studies with varying spatial coverage, resolution, and reference systems. The subcomponents are named: Macro level, Meso level, and Micro level. - Macro Level Analysis - `Spatial Coverage`: Continental United States - `Spatial Resolution`: EPA Regions (aggregations of states) - `Spatial Reference System`: ESRI:102003 - Meso Level Analysis - `Spatial Coverage`: EPA Region 4 - `Spatial Resolution`: States (first admin level) - `Spatial Reference System`: ESRI:102003 - Micro Level Analysis - `Spatial Coverage`: Florida - `Spatial Resolution`: Counties (second admin level) - `Spatial Reference System`: EPSG:3086 A table may be used in place of a hierarchical list. For example: | | Macro Level | Meso Level | Micro Level | | :----------------------: | :------------: | :----------: | :------------: | | Spatial Coverage | Continental US | EPA Region 4 | Florida | | Spatial Resolution | EPA Regions | States ADM_1 | Counties ADM_2 | | Spatial Reference System | ESRI:102003 | ESRI:102003 | EPSG:3086 | 5.1.2 Replication studies with different spatio-temporal coverages If the study is replication of a prior study with different spatial or temporal characteristics from the prior study, then the first block of metadata should describe the replication study spatio-temporal characteristics. Following the replication study metadata and prior to the Study design section, add a sub-section for the original (prior) study spatio-temporal characteristics. For example: #### Original study spatio-temporal metadata - `Spatial Coverage`: extent of original study - `Spatial Resolution`: resolution of original study - `Spatial Reference System`: spatial reference system of original study - `Temporal Coverage`: temporal extent of original study - `Temporal Resolution`: temporal resolution of original study 5.2 Study design This section describes the archetypal study design to be used and specifies clear hypotheses or research questions. Describe how the study relates to prior literature, e.g. is it an original study or a metascience study (one of: meta-analysis study, reproduction study, reanalysis study, or replication study)? Also describe the original study archetype, e.g. is it observational, experimental, quasi-experimental, or exploratory? Enumerate specific hypotheses to be tested or research questions to be investigated here, and specify the type of method, statistical test or model to be used on the hypothesis or question. For example: &gt; H1: Hypothesis number one `H1` will be tested with a linear regression model. If the study is a reproduction, reanalysis, replication, or meta-analysis, use prefixes to differentiate between the original study hypothesis and the meta-science hypothesis. We recommend the following prefixes: OR-H: original hypothesis MA-H: meta-analysis hypothesis RPr-H: reproduction hypothesis RA-H: reanalysis hypothesis RPl-H: replication hypothesis enumerate the original study hypotheses to be analyzed and the type of test or model to be used for the hypothesis. These can be numbered with the prefix OR-H for “Original Hypothesis”, e.g. &gt; OR-H1: Median home prices of census tracts are dependent upon the distance from the central business district (CBD) in Chicago, Illinois. OR-H1 was tested with a linear regression model having median home prices as the dependent variable and distance between the central business district and the census tract centroid as the independent variable. &gt; RPl-H1: The coefficient of distance from CBD in Buffalo, New York will have equivalent direction and magnitude to the coefficient of distance from the CBD in Chicago, Illinois. RPl-H1 will be tested by substituting data for Chicago, Illinois with data for Buffalo, New York and repeating the linear regression of OR-H1. The original study used a Spearman’s Rho rank correlation test for OR-H1. 5.3 Planned deviations of metascience studies If the study is a metascience study (i.e. a reproduction, reanalysis, or replication), then most of the analysis plan has simply articulated your best understanding of the prior study. However, to different degrees, each metascience study may intend to alter some parameters of the prior study. These alterations should be labelled planned deviation and include a rationale for the deviation. Categorize deviations for reproduction if the aim of the deviation is still to reproduce the original methodology and original results. Categorize deviations as for reanalysis if the aim is to alter a methodological parameter of the study in order to compare results, e.g. as a test of sensitivity, uncertainty, or robustness. Categorize deviations as for replication if the aim is to alter the spatial-temporal coverage of the study or to otherwise repeat the study methodology with new data/observations. For example, a recent reproduction study of ours contained a planned deviation with no expectation of altering the data, methods, or results: **Planned deviation for reproduction**: Although the original study used SPSS and SaTScan for analysis, we will attempt to reproduce the study using R and its spatialepi and geepack packages for equivalent analysis using an open source software environment. 5.4 Preregister your analysis plan on OSF In the Open Science Foundation ecosystem of open science infrastructure, registration means to make and archive a permanent record of the research project at one moment in time. When you register an analysis plan or report associated with an OSF project, a copy of the entire project is made to accompany the registration. A preregistration should be a registration of the research prior to data collection and analysis. As you are ready to preregister your plan, remember to update the Date modified field in your plan and on your project readme.md front matter. When you are ready to register your analysis plan, log into OSF and navigate to your project. Open the Registrations menu and begin a New registration. OSF contains several registration templates. To use our own template, select Open-Ended Registration. Even an open-ended registration requires standard metadata. Fortunately, OSF pulls this metadata from the umbrella project, including title, description, contributors, category, and license. Curiously, this is the first opportunity to enter a subject, which will likely be one of: - Social and Behavioral Sciences  Geography  (Geographic Information Sciences, Human Geography, Nature and Society Relations, Other Geography, Physical and Environmental Geography, Remote Sensing, Spatial Science)pro - Social and Behavioral Sciences  Environmental Studies  … Following Metadata, the Summary should be a short description of the file you are uploading, simply to describe that it is a pre-analysis plan registration. If you are updating a prior plan, the major updates and rationale should be noted in this summary. Upload your pre-analysis plan pdf file as a supplementary file. Finally, Review the registration and confirm its accuracy before moving on to Register the plan. At this stage you will have the option to make the publication public immediately or enter the registration into an embargo with a specified end date. The embargo option means that the registration will be locked after you Submit it, but it will remain private until the embargo date, after which it will be made public. A submitted registration cannot be deleted or revoked even if it is in embargo: your only recourse for errors in the registration is to make a new registration to supersede the prior one. All of the other admin contributors on the registration will have an option to revoke the registration in the first 48 hours after submission. Once you have registered the pre-analysis plan, remember to copy the DOI and add the DOI link in the Related to section of the compendium readme.md file. Note: If you need your project and the associated registration to identify specific files in your research compendium as specific types of resources, then prior to registration you can navigate to the file and edit file-level metadata. See these instructions: https://help.osf.io/article/569-add-metadata-to-an-osf-file 5.5 Amending the analysis plan registration Once registered, an analysis plan is locked in as a permanent record of your research plan at a particular time. However, plans change! All changes to plans between a pre-analysis registration and a post-analysis report must be documented as unplanned deviations so that results stemming from those changes may be interpreted appropriately. Keeping in mind that the purpose of a preregistration is to reduce increase transparency for the purpose of reducing bias and p-hacking in the research process, a revised preregistration will only be useful if the researcher(s), based on prior observations of the data, can still specify a relatively unbiased analysis plan. If the analysis plan is being amended on the fly based on reactions to data observations, it is better to record the amendments as unplanned deviations in a final report, rather than to falsely claim the changes as unbiased amendments to the analysis plan prior to the study. If you need to make significant changes to an analysis plan and the amended plan is not already substantially biased by prior analyses and data observations, then it is time to amend the analysis plan and update the preregistration. In case you need to amend the analysis plan, you may edit any section of the prior plan and add an explanation of the changes. Be sure to update the Date modified field of the study metadata. We recommend adding a second-level section titled “Rationale for analysis plan update” to the end of the analysis plan, immediately following the Integrity Statement section. This new section should begin with a statement including the plan version and the prior registration DOI link, followed by the rationale for the update in one or more paragraphs. Also include a paragraph discussing any change in prior observations of the data since the prior preregistration, or a statement of no change to prior observations of data. ### Rationale for updated report This is the second version of the analysis plan preregistration, superseding the first version, registered at https://doi.org/10.17605/OSF.IO/647EX on July 22, 2022. The rationale for this update is... Our prior observations of the data have changed/not changed. Since the prior version, we have now... 5.6 Resources Dublin Core: ISO 19115: OSF Preregistration resources: https://www.cos.io/initiatives/prereg Guide to create registrations: https://help.osf.io/article/345-create-registrations "],["data-observation-and-curation.html", "Chapter 6 Data Observation and Curation 6.1 Data Management 6.2 Collect preliminary data 6.3 Updating the analysis plan", " Chapter 6 Data Observation and Curation 6.1 Data Management Store all of your research data in the data subdirectories. It is recommended that raw data not be altered once downloaded or collected. Maintaining a separate raw data file facilitates reproducibility be preserving a common point of analytical origin. It is similarly recommended that whenever possible data processing, transformation, or manipulation be completed with code as this practice facilitates re-analysis and reduces opportunities of confusion. Complete the data_metadata.csv file indexing each raw and derived data file, including the fields: path: the path to the data folder, likely one of: raw\\private, raw\\public, derived\\private or derived\\public name: the file name, including extension metadata: list of metadata files for this data source, stored in the data\\metadata folder. These may include ISO-191** or FGDC standard XML files, data dictionaries, licenses or attributions, user guides, webpage printouts, etc. status: which may be included for data included in the repository or create or acquire for data that must created or acquired, derived for data that will be generated by code from other data files, simulated for data that replaces the true research data with a simulated data due to confidentiality or legal constraints, and unavailable for data that cannot be shared or reproduced in any way. description: very brief description of the dataset. Researchers are strongly encouraged to include additional metadata in the metadata folder. Further information about the procedures used to create data with ‘status = derive’ should be maintained in the procedure_metadata.csv. See more about metadata in the engaging with data section of the previous chapter. 6.2 Collect preliminary data metadata! code/scripts for data acquisition directory structure for data scratch (not tracked) raw / public raw / private (not tracked) derived / public derived / private (not tracked) file size limits for GitHub / GitLab Processing Access – Private Public Raw RPri RPub Derived DPri DPub 6.2.1 Raw private data Store raw data in this folder as it is collected or downloaded if the data cannot be publicly redistributed. For example, data versioning and sharing my be restricted because of large file sizes, licensing, ethics, privacy, or confidentiality. Best practices are to include code to automate the process of downloading or simulating raw private data in the first step of the methods, or to include instructions here for accessing any private or restricted-access data. This folder is ignored by Git versioning with the exception of this readme.md file by the following lines in .gitignore # Ignore contents of private folder, with the exception of its readme file private/** !private/readme.md 6.2.2 Caution: Dealing with large files GitHub will not store files larger than 100mb These should be placed in private directories so that they are not tracked by Git or uploaded to GitHub OSF and Figshare both allow for larger file storage options, so you may store large files on those services and write code for downloading those files to private directories as the analysis runs. Significant data sources could be registered with their own DOI links. Git and GitHub are not designed to track or store large files over 100 mb. If you have accidentally attempted to commit changes with large files, do this… INSTRUCTIONS 6.3 Updating the analysis plan You will likely encounter unexpected challenges and the need to change your original, pre-analysis registration plan. This is normal: just be diligent about updating your analysis plan, cataloguing deviations from the original plan, and committing changes to the repository. Document unplanned deviations as they occur in the analysis plan. If the study is a metascience study, then categorize unplanned deviations for reproduction if the aim of the deviation is still to reproduce the original methodology and original results. Categorize deviations as for reanalysis if the aim is to alter a methodological parameter of the study to compare results, e.g. as a test of sensitivity, uncertainty, or robustness. Categorize deviations as for replication if the aim is to alter the spatial-temporal coverage of the study or to otherwise repeat the study methodology with new data/observations. For full transparency, document both the rationale and the form of each deviation. "],["analysis-and-validation.html", "Chapter 7 Analysis and Validation 7.1 Changes to the pre-analysis plan registration 7.2 Coding and Code style 7.3 Intermediary outputs 7.4 Function parameters 7.5 Store publication-ready outputs in the results directory", " Chapter 7 Analysis and Validation 7.1 Changes to the pre-analysis plan registration Researchers will likely encounter the need to deviate from the registered pre-analysis plan. save a version of the text file used to write and register the pre-analysis plan major changes to the plan may require updating that document, rendering a new PDF and updating the registration on OSF. QUESTION: when is a change to the registered pre-analysis plan big enough to require an update on OSF? - if an uncertainty or ambiguity and contingencies for that were already articulated in the pre-analysis plan, there is no need to update the plan. - if alterations to the pre-analysis plan may ultimately impact the interpretation of claims made by the research, the changes should be registered on OSF. in parallel, all changes (minor and major) should be identified with bold text or subheadings in the narrative of the analytical plan as research progresses. Our suggested labels for changes include: update the preanalysis plan 7.2 Coding and Code style R should follow Tidyverse style: https://style.tidyverse.org/ and there are two great addins: - stylr formats your code for you https://styler.r-lib.org/ - lintr checks your code formatting: https://github.com/jimhester/lintr Python should follow PEP 8 Style: https://www.python.org/dev/peps/pep-0008/ 7.3 Intermediary outputs It is possible, but unwise, to encode the entire workflow in a computational notebook without saving intermediary results. When should intermediary results be saved, how, and why? There are at least five major situations in which saving data results is paramount for increasing the reproducibility of your study: Immediately following code to download or create data that can be redistributed, the data should be saved in the raw\\public folder. As soon as confidential, proprietary, or very large data sources has been processed enough to redistribute, the data should be saved to the derived\\public folder. After very computationally intensive processes, results should be saved to the derived\\public folder. If any external software or process is used to manipulate the data, then the data should be saved to the derived\\public folder just prior to the external process and just after the external process. Any process outside of the main computational notebook should be documented in notebook narrative and/or separate procedure code or protocol files. If any randomization or stochastic model is applied in the analysis, the results of the random aspect of the analysis should be saved to the derived\\public folder. The computational notebook should include code both to save and to (re)load intermediary data files, and the recommended practice is to: 1. save in formats with open standards without encoding the data object name 2. read in with functions that assign a data object name Recommended open standard file types include .csv, .json and .gpkg. For example, while working in R, it is best to use the .RDS file type to store individual data objects without the data object name rather than relying on .RData files potentially encompassing multiple data objects and encoding their names. Saving data files agnostic to the object name in R or Python adds clarity to the provenance of data as it is manipulated by the research workflow and allows researchers to modify the code and compare results of replications or reanalyses. Researchers can encode data saving and loading procedures in separate code blocks and modify the code chunk options for optimal reproducibiliy. For example, it is not necessary to query an API every time a computational notebook is run: the code chunk for an API query can be set to eval = FALSE while the code chunk to load the resulting data can be set to eval = TRUE. See RMarkdown code chunk options: https://rpubs.com/Lingling912/870659 and Jupyter code block options 7.4 Function parameters While it may be convenient to use the default parameters for functions while coding the script for a research project, this practice makes it more difficult for other researchers to understand the code and may be vulnerable to changes in the required packages over time. If a parameter is important, best practice is to include it in the function call. As an example, functions for classifying continuous data for the purpose of choropleth mapping may contain a default number of classes, but it is helpful to choose and declare that default explicitly in the research code rather than requiring others to look up function defaults. The result of declaring parameters will be more intentional research design and more legible code. 7.5 Store publication-ready outputs in the results directory Store publication-ready final research outputs, e.g. figures, tables, or other media for publications and presentations in the results directory. Although these types of results will ideally be embedded as outputs in computational notebooks, most researchers will also find it convenient to share and repurpose individual figures and tables for blogs, web designs, presentations, and more. Therefore, it makes sense to add a final block of code to computational studies to output important results to separate files. As you do so, complete the results_metadata.csv file indexing each results file, including the fields: path: the path from results folder, e.g., figures, other, or tables name: the file name description: very brief description or figure title Wise researchers may even edit this results_metadata.csv file in their computational notebook! "],["dissemination.html", "Chapter 8 Dissemination", " Chapter 8 Dissemination Export your work to LaTeX, using an appropriate template for your intended publication Register a preprint of your work Submit for peer review Track versions using GitHub include citations for the open science data and software in your study, e.g. by using the citation() function in R "],["preservation.html", "Chapter 9 Preservation 9.1 LICENSE 9.2 CITATION 9.3 versioning", " Chapter 9 Preservation 9.1 LICENSE Verify that you have used an appropriate open access license in your LICENSE file Either here or in the provocation phase, or both, discuss reasons for the BSD 3.0 license. 9.2 CITATION Verify that the CITATION.cff file on GitHub has appropriate citation information and that the “cite this repository” feature correctly generates an appropriate citation for your work. https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-citation-files 9.3 versioning Commit a final version with tagged release on GitHub Register a final version on OSF large files can be uploaded to OSF or attatched to a release on GitHub "],["reproduction.html", "Chapter 10 Reproduction 10.1 R 10.2 Python 10.3 Acknowledgement", " Chapter 10 Reproduction The whole concept behind open science is that other researchers or students will be able to reproduce, replicate, and extend scientific studies after they are published. This chapter is devoted to that epilogue: how to engage with a published open science study. The repurposing of a scientific study will normally start with reproducing the original study results– a process that will differ depending on the computational environment and original materials. We hope that by standardizing a research template, we will have eased the work of reproducing the study. This chapter provides some guidance for reproducing a study that is specific to research published with an open science workflow and conforming to version 2.0 or greater of the HEGSRR-Template. The chapter essentially outlines methods for reproducing the computational environment of a prior study. 10.1 R How to reproduce in R. Sentences on new lines, breaks between paragraphs. 10.2 Python How to reproduce in R. Sentences on new lines, breaks between paragraphs. 10.3 Acknowledgement Special thanks to Yifei Luo for contributions to this chapter. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
